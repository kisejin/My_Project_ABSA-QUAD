{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-13T13:02:47.832953Z",
     "iopub.status.busy": "2023-12-13T13:02:47.832570Z",
     "iopub.status.idle": "2023-12-13T13:03:10.889224Z",
     "shell.execute_reply": "2023-12-13T13:03:10.887818Z",
     "shell.execute_reply.started": "2023-12-13T13:02:47.832923Z"
    },
    "id": "viF-XbCLzof2",
    "outputId": "ef428e0d-2851-464b-e246-92f5c004a823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers pytorch_lightning sentencepiece sentence-transformers easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-13T03:04:58.473355Z",
     "iopub.status.busy": "2023-12-13T03:04:58.472643Z",
     "iopub.status.idle": "2023-12-13T03:05:05.096878Z",
     "shell.execute_reply": "2023-12-13T03:05:05.096035Z",
     "shell.execute_reply.started": "2023-12-13T03:04:58.473322Z"
    },
    "id": "30AX4GDrz6hA",
    "outputId": "49e567bb-e2db-41c0-e37d-84a36f795c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'My_Project_ABSA-QUAD'...\n",
      "remote: Enumerating objects: 183, done.\u001b[K\n",
      "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
      "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
      "remote: Total 183 (delta 92), reused 105 (delta 63), pack-reused 27\u001b[K\n",
      "Receiving objects: 100% (183/183), 4.25 MiB | 19.50 MiB/s, done.\n",
      "Resolving deltas: 100% (95/95), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kisejin/My_Project_ABSA-QUAD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T02:48:36.992222Z",
     "iopub.status.busy": "2023-12-13T02:48:36.991963Z",
     "iopub.status.idle": "2023-12-13T02:48:38.053395Z",
     "shell.execute_reply": "2023-12-13T02:48:38.052300Z",
     "shell.execute_reply.started": "2023-12-13T02:48:36.992196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:15.489571Z",
     "iopub.status.busy": "2023-12-13T13:03:15.489350Z",
     "iopub.status.idle": "2023-12-13T13:03:15.506135Z",
     "shell.execute_reply": "2023-12-13T13:03:15.504604Z",
     "shell.execute_reply.started": "2023-12-13T13:03:15.489523Z"
    },
    "id": "4pezMNQbz_IG",
    "outputId": "2f26a677-0bf2-46b5-d8cb-be9114c81205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/My_Project_ABSA-QUAD\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/My_Project_ABSA-QUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T02:48:38.064045Z",
     "iopub.status.busy": "2023-12-13T02:48:38.063830Z",
     "iopub.status.idle": "2023-12-13T02:48:38.256567Z",
     "shell.execute_reply": "2023-12-13T02:48:38.255101Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.064025Z"
    },
    "id": "lYPjT_yCJRxT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Import drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.257184Z",
     "iopub.status.idle": "2023-12-13T02:48:38.257469Z",
     "shell.execute_reply": "2023-12-13T02:48:38.257346Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.257332Z"
    },
    "id": "A-c1mSrfJcnd"
   },
   "outputs": [],
   "source": [
    "%cp /content/drive/MyDrive/Model_ProjectHotel/outputs.zip /content/\n",
    "%cd /content/\n",
    "!unzip outputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.258834Z",
     "iopub.status.idle": "2023-12-13T02:48:38.259098Z",
     "shell.execute_reply": "2023-12-13T02:48:38.258987Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.258984Z"
    },
    "id": "Y34inF7I0Ofu",
    "outputId": "b3cd2376-9fca-45c7-c530-4604da4d1faa"
   },
   "outputs": [],
   "source": [
    "# --model_name_or_path t5-base \\\n",
    "# general task: asqp\n",
    "# genral train_batch_size, eval_batch_size: 16\n",
    "\n",
    "!python main.py --task tasd \\\n",
    "            --dataset mydata \\\n",
    "            --model_name_or_path t5-base \\\n",
    "            --n_gpu 0 \\\n",
    "            --do_train \\\n",
    "            --do_direct_eval \\\n",
    "            --train_batch_size 16 \\\n",
    "            --gradient_accumulation_steps 1 \\\n",
    "            --eval_batch_size 16 \\\n",
    "            --learning_rate 3e-4 \\\n",
    "            --num_train_epochs 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to file zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.260049Z",
     "iopub.status.idle": "2023-12-13T02:48:38.260299Z",
     "shell.execute_reply": "2023-12-13T02:48:38.260178Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.260178Z"
    },
    "id": "NQm5e8GE4_Wf",
    "outputId": "259e146c-afe5-44f7-ef8b-b3170823ae72"
   },
   "outputs": [],
   "source": [
    "# zip file\n",
    "# import shutil\n",
    "# shutil.make_archive('/notebooks/output', 'zip', '/notebooks/My_Project_ABSA-QUAD/outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data loader , data preprocessing\n",
    "* Must run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:20.444287Z",
     "iopub.status.busy": "2023-12-13T13:03:20.443877Z",
     "iopub.status.idle": "2023-12-13T13:03:21.925914Z",
     "shell.execute_reply": "2023-12-13T13:03:21.924394Z",
     "shell.execute_reply.started": "2023-12-13T13:03:20.444255Z"
    },
    "id": "J6XvvMhUPMMR"
   },
   "outputs": [],
   "source": [
    "# File data_utils.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# This script contains all data transformation and reading\n",
    "\n",
    "import random\n",
    "import re\n",
    "import regex as reg\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "senttag2word = {\"POS\": \"positive\", \"NEG\": \"negative\", \"NEU\": \"neutral\"}\n",
    "senttag2opinion = {\"POS\": \"great\", \"NEG\": \"bad\", \"NEU\": \"ok\"}\n",
    "sentword2opinion = {\"positive\": \"great\", \"negative\": \"bad\", \"neutral\": \"ok\"}\n",
    "\n",
    "aspect_cate_list = [\n",
    "    \"location general\",\n",
    "    \"food prices\",\n",
    "    \"food quality\",\n",
    "    \"food general\",\n",
    "    \"ambience general\",\n",
    "    \"service general\",\n",
    "    \"restaurant prices\",\n",
    "    \"drinks prices\",\n",
    "    \"restaurant miscellaneous\",\n",
    "    \"drinks quality\",\n",
    "    \"drinks style_options\",\n",
    "    \"restaurant general\",\n",
    "    \"food style_options\",\n",
    "    \"facility\",\n",
    "    \"amenity\",\n",
    "    \"service\",\n",
    "    \"experience\",\n",
    "    \"branding\",\n",
    "    \"loyalty\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_line_examples_from_file(data_path, silence=False):\n",
    "    \"\"\"\n",
    "    Read data from file, each line is: sent####labels\n",
    "    Return List[List[word]], List[Tuple]\n",
    "    \"\"\"\n",
    "    id_users, sents, labels = [], [], []\n",
    "    with open(data_path, \"r\", encoding=\"UTF-8\") as fp:\n",
    "        words, labels = [], []\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            if line != \"\":\n",
    "                words, tuples = line.split(\"####\")\n",
    "                if tuples != \"\":\n",
    "                    labels.append(eval(tuples))\n",
    "                else:\n",
    "                    words = line\n",
    "                # Get id user at the beginning of line if it exist\n",
    "                words = words.split()\n",
    "                if len(words[0].split(',', 1)) > 1:\n",
    "                    id_user, word = words[0].split(',',1)\n",
    "                    words[0] = word\n",
    "                    id_users.append(int(id_user))\n",
    "                sents.append(words)\n",
    "\n",
    "    if silence:\n",
    "        print(f\"Total examples = {len(sents)}\")\n",
    "    return id_users, sents, labels\n",
    "\n",
    "\n",
    "def get_para_aste_targets(sents, labels):\n",
    "    targets = []\n",
    "    for i, label in enumerate(labels):\n",
    "        all_tri_sentences = []\n",
    "        for tri in label:\n",
    "            # a is an aspect term\n",
    "            if len(tri[0]) == 1:\n",
    "                a = sents[i][tri[0][0]]\n",
    "            else:\n",
    "                start_idx, end_idx = tri[0][0], tri[0][-1]\n",
    "                a = \" \".join(sents[i][start_idx : end_idx + 1])\n",
    "\n",
    "            # b is an opinion term\n",
    "            if len(tri[1]) == 1:\n",
    "                b = sents[i][tri[1][0]]\n",
    "            else:\n",
    "                start_idx, end_idx = tri[1][0], tri[1][-1]\n",
    "                b = \" \".join(sents[i][start_idx : end_idx + 1])\n",
    "\n",
    "            # c is the sentiment polarity\n",
    "            c = senttag2opinion[tri[2]]  # 'POS' -> 'good'\n",
    "\n",
    "            one_tri = f\"It is {c} because {a} is {b}\"\n",
    "            all_tri_sentences.append(one_tri)\n",
    "        targets.append(\" [SSEP] \".join(all_tri_sentences))\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_para_tasd_targets(sents, labels):\n",
    "    targets = []\n",
    "    for label in labels:\n",
    "        all_tri_sentences = []\n",
    "        for triplet in label:\n",
    "\n",
    "            at, ac, sp = triplet\n",
    "            at, ac, sp = at.lower(), ac.lower(), sp.lower()\n",
    "\n",
    "            # Remove special characters in the end of aspect term\n",
    "            at = re.sub(r'([^\\w\\s]|_)+(?=\\s|$)', '', at)\n",
    "\n",
    "            man_ot = sentword2opinion[sp]  # 'positive' -> 'great'\n",
    "\n",
    "            if at == \"NULL\" or at == '':\n",
    "                at = \"it\"\n",
    "\n",
    "            one_tri = f\"{ac} is {man_ot} because {at} is {man_ot}\"\n",
    "\n",
    "            # if at == '' or ac == '' or sp == '':\n",
    "            #   print(f\"Triplet: {triplet} \")\n",
    "\n",
    "            all_tri_sentences.append(one_tri)\n",
    "\n",
    "        target = \" [SSEP] \".join(all_tri_sentences)\n",
    "        targets.append(target)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_para_asqp_targets(sents, labels):\n",
    "    \"\"\"\n",
    "    Obtain the target sentence under the paraphrase paradigm\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    for label in labels:\n",
    "        all_quad_sentences = []\n",
    "        for quad in label:\n",
    "            at, ac, sp, ot = quad\n",
    "            at, ac, sp, ot = at.lower(), ac.lower(), sp.lower(), ot.lower()\n",
    "            man_ot = sentword2opinion[sp]  # 'POS' -> 'good'\n",
    "\n",
    "            if at == \"NULL\":  # for implicit aspect term\n",
    "                at = \"it\"\n",
    "\n",
    "            one_quad_sentence = f\"{ac} is {man_ot} because {at} is {ot}\"\n",
    "            all_quad_sentences.append(one_quad_sentence)\n",
    "\n",
    "        target = \" [SSEP] \".join(all_quad_sentences)\n",
    "        targets.append(target)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_transformed_io(data_path, data_dir, task = 'asqp'):\n",
    "    \"\"\"\n",
    "    The main function to transform input & target according to the task\n",
    "    \"\"\"\n",
    "    id_user, sents, labels = read_line_examples_from_file(data_path)\n",
    "\n",
    "    # the input is just the raw sentence\n",
    "    inputs = [s.copy() for s in sents]\n",
    "    targets = []\n",
    "\n",
    "    if labels:\n",
    "        if task == \"aste\":\n",
    "            targets = get_para_aste_targets(sents, labels)\n",
    "        elif task == \"tasd\":\n",
    "            targets = get_para_tasd_targets(sents, labels)\n",
    "        elif task == \"asqp\":\n",
    "            targets = get_para_asqp_targets(sents, labels)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return id_user, inputs, targets\n",
    "\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, data_type, task, max_len=128):\n",
    "        # './data/rest16/train.txt'\n",
    "        self.data_path = f\"data/{data_dir}/{data_type}.txt\"\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.id_users = []\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self.task = task\n",
    "\n",
    "        self._build_examples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\n",
    "            \"attention_mask\"\n",
    "        ].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\n",
    "            \"attention_mask\"\n",
    "        ].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids,\n",
    "            \"source_mask\": src_mask,\n",
    "            \"target_ids\": target_ids,\n",
    "            \"target_mask\": target_mask,\n",
    "        }\n",
    "\n",
    "    def _build_examples(self):\n",
    "        id_user, inputs, targets = get_transformed_io(\n",
    "            self.data_path, self.data_dir, self.task\n",
    "        )\n",
    "\n",
    "        check = 1 if len(id_user) > 0 else 0\n",
    "        for i in range(len(inputs)):\n",
    "            # change input and target to two strings\n",
    "            input = \" \".join(inputs[i])\n",
    "            target = targets[i]\n",
    "\n",
    "            tokenized_input = self.tokenizer.batch_encode_plus(\n",
    "                [input],\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokenized_target = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            if check:\n",
    "                self.id_users.append(id_user[i])\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.targets.append(tokenized_target)\n",
    "            \n",
    "\n",
    "class ABSADataset_nolabel(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, data_type, task, max_len=128):\n",
    "        # './data/rest16/train.txt'\n",
    "        self.data_path = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.id_users = []\n",
    "        self.inputs = []\n",
    "\n",
    "        self.task = task\n",
    "\n",
    "        self._build_examples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\n",
    "            \"attention_mask\"\n",
    "        ].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids,\n",
    "            \"source_mask\": src_mask,\n",
    "        }\n",
    "\n",
    "    def _build_examples(self):\n",
    "        id_user, inputs, _ = get_transformed_io(\n",
    "            self.data_path, self.data_dir, self.task\n",
    "        )\n",
    "\n",
    "        check = 1 if len(id_user) > 0 else 0\n",
    "        for i in range(len(inputs)):\n",
    "            # change input and target to two strings\n",
    "            input = \" \".join(inputs[i])\n",
    "\n",
    "            tokenized_input = self.tokenizer.batch_encode_plus(\n",
    "                [input],\n",
    "                max_length=self.max_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            if check:\n",
    "                self.id_users.append(id_user[i])\n",
    "            self.inputs.append(tokenized_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data postprocessing and evaluation\n",
    "* Must run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:25.103677Z",
     "iopub.status.busy": "2023-12-13T13:03:25.102962Z",
     "iopub.status.idle": "2023-12-13T13:03:29.152861Z",
     "shell.execute_reply": "2023-12-13T13:03:29.151929Z",
     "shell.execute_reply.started": "2023-12-13T13:03:25.103627Z"
    },
    "id": "8b_1_8imPM1D"
   },
   "outputs": [],
   "source": [
    "# File eval_utils.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# This script handles the decoding functions and performance measurement\n",
    "\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "sentiment_word_list = ['positive', 'negative', 'neutral']\n",
    "opinion2word = {'great': 'positive', 'bad': 'negative', 'ok': 'neutral'}\n",
    "opinion2word_under_o2m = {'good': 'positive', 'great': 'positive', 'best': 'positive',\n",
    "                          'bad': 'negative', 'okay': 'neutral', 'ok': 'neutral', 'average': 'neutral'}\n",
    "numopinion2word = {'SP1': 'positive', 'SP2': 'negative', 'SP3': 'neutral'}\n",
    "\n",
    "\n",
    "def extract_spans_para(task, seq, seq_type):\n",
    "    quads = []\n",
    "    sents = [s.strip() for s in seq.split('[SSEP]')]\n",
    "\n",
    "    # Replace subword of \"[[SSEP]]\" -> ''\n",
    "    list_Words = [ '[SSEP', '[SSE', '[SS', '[S', '[']\n",
    "    big_regex = re.compile('|'.join(map(re.escape, list_Words)))\n",
    "\n",
    "    sents = [big_regex.sub(\"\", s).strip() for s in sents]\n",
    "\n",
    "\n",
    "\n",
    "    if task == 'aste':\n",
    "        for s in sents:\n",
    "            # It is bad because editing is problem.\n",
    "            try:\n",
    "                c, ab = s.split(' because ')\n",
    "                c = opinion2word.get(c[6:], 'nope')    # 'good' -> 'positive'\n",
    "                a, b = ab.split(' is ')\n",
    "            except ValueError:\n",
    "                # print(f'In {seq_type} seq, cannot decode: {s}')\n",
    "                a, b, c = '', '', ''\n",
    "            quads.append((a, b, c))\n",
    "\n",
    "    elif task == 'tasd':\n",
    "        for s in sents:\n",
    "            # food quality is bad because pizza is bad.\n",
    "\n",
    "            # print(f'\\nSentence: {s}')\n",
    "            try:\n",
    "                # ac_sp, at_sp = s.split(' because ')\n",
    "                # Check sentence is consist two or more than 'because'\n",
    "                s_tmp = s.split(' because ')\n",
    "                # ac_sp, at_sp = s_tmp\n",
    "                if len(s_tmp) > 2:\n",
    "                    ac_sp = s_tmp[0]\n",
    "                    at_sp = ' because '.join(sent for sent in s_tmp[1:])\n",
    "                else:\n",
    "                    ac_sp, at_sp = s_tmp\n",
    "\n",
    "\n",
    "                #  Extract sentiment level in statement 1 of sentence\n",
    "                ac, sp = ac_sp.split(' is ')\n",
    "\n",
    "                # Check statement 2 consist two or more than \"is\"\n",
    "                at_sp2 = at_sp.split(' is ')\n",
    "                if len(at_sp2) > 2:\n",
    "                    at = ' is '.join(sent for sent in at_sp2[:-1])\n",
    "                    sp2 = at_sp2[-1]\n",
    "                else:\n",
    "                    at, sp2 = at_sp2\n",
    "\n",
    "                sp = opinion2word.get(sp, 'nope')\n",
    "                sp2 = opinion2word.get(sp2, 'nope')\n",
    "                if sp != sp2:\n",
    "                    print(f'Sentiment polairty of AC({sp}) and AT({sp2}) is inconsistent!')\n",
    "                    print(f'Sentence: {s}\\n')\n",
    "\n",
    "                # if the aspect term is implicit\n",
    "                if at.lower() == 'it':\n",
    "                    at = 'NULL'\n",
    "\n",
    "            except ValueError:\n",
    "                # print(f'In {seq_type} seq, cannot decode: {s}')\n",
    "                ac, at, sp = '', '', ''\n",
    "\n",
    "            quads.append((ac, at, sp))\n",
    "\n",
    "\n",
    "    elif task == 'asqp':\n",
    "        for s in sents:\n",
    "            # food quality is bad because pizza is over cooked.\n",
    "            try:\n",
    "                ac_sp, at_ot = s.split(' because ')\n",
    "                ac, sp = ac_sp.split(' is ')\n",
    "                at, ot = at_ot.split(' is ')\n",
    "\n",
    "                # if the aspect term is implicit\n",
    "                if at.lower() == 'it':\n",
    "                    at = 'NULL'\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # print(f'In {seq_type} seq, cannot decode: {s}')\n",
    "                    pass\n",
    "                except UnicodeEncodeError:\n",
    "                    # print(f'In {seq_type} seq, a string cannot be decoded')\n",
    "                    pass\n",
    "                ac, at, sp, ot = '', '', '', ''\n",
    "\n",
    "            quads.append((ac, at, sp, ot))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return quads\n",
    "\n",
    "\n",
    "def length_of_null_quads(list_span: list):\n",
    "    return sum(1 for x in list_span if x[0] == '')\n",
    "\n",
    "\n",
    "\n",
    "def compute_f1_scores(pred_pt, gold_pt):\n",
    "    \"\"\"\n",
    "    Function to compute F1 scores with pred and gold quads\n",
    "    The input needs to be already processed\n",
    "    \"\"\"\n",
    "    # number of true postive, gold standard, predictions\n",
    "    n_tp, n_gold, n_pred, n_gold_null, n_pred_null = 0, 0, 0, 0, 0\n",
    "    sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    for i in range(len(pred_pt)):\n",
    "            # n_gold += len(gold_pt[i])\n",
    "            # n_pred += len(pred_pt[i])\n",
    "\n",
    "            # for t in pred_pt[i][]:\n",
    "            #     if t in gold_pt[i]:\n",
    "            #         n_tp += 1\n",
    "\n",
    "\n",
    "\n",
    "            ####### CONFIG #######\n",
    "            n_gold += len(gold_pt[i])\n",
    "            n_gold_null += length_of_null_quads(gold_pt[i])\n",
    "\n",
    "            n_pred += len(pred_pt[i])\n",
    "            n_pred_null += length_of_null_quads(pred_pt[i])\n",
    "\n",
    "            for p, g in zip(pred_pt[i], gold_pt[i]):\n",
    "                  if p[0] != '' and g[0] != '':\n",
    "\n",
    "                      if p == g:\n",
    "                          n_tp += 1\n",
    "                      else:\n",
    "                          # Similarity between gold and pred by bert embedding\n",
    "                          encode_gold = sbert_model.encode(g[1])\n",
    "                          encode_pred = sbert_model.encode(p[1])\n",
    "\n",
    "                          # Define cosine similarity\n",
    "                          cos = torch.nn.CosineSimilarity(dim = 0, eps=1e-6)\n",
    "                          sim = cos(torch.Tensor(encode_gold), torch.Tensor(encode_pred)).item()\n",
    "\n",
    "                          if sim >= 0.4 and p[0].lower() == g[0].lower() and p[2:]== g[2:]:\n",
    "                              n_tp += 1\n",
    "\n",
    "            ####### ........ #######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n_gold = n_gold - n_gold_null\n",
    "    n_pred = n_pred - n_pred_null\n",
    "\n",
    "    print(f\"number of gold spans: {n_gold}, predicted spans: {n_pred}, hit: {n_tp}\")\n",
    "    precision = float(n_tp) / float(n_pred) if n_pred != 0 else 0\n",
    "    recall = float(n_tp) / float(n_gold) if n_gold != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision != 0 or recall != 0 else 0\n",
    "    scores = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_scores(pred_seqs, gold_seqs, sent, task = 'asqp'):\n",
    "    \"\"\"\n",
    "    Compute model performance\n",
    "    \"\"\"\n",
    "    assert len(pred_seqs) == len(gold_seqs)\n",
    "    num_samples = len(gold_seqs)\n",
    "\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        gold_list = extract_spans_para(task, gold_seqs[i], 'gold')\n",
    "        pred_list = extract_spans_para(task, pred_seqs[i], 'pred')\n",
    "\n",
    "        # print(gold_seqs[i])\n",
    "        # print(gold_list)\n",
    "        all_labels.append(gold_list)\n",
    "        all_preds.append(pred_list)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    scores = compute_f1_scores(all_preds, all_labels)\n",
    "    print(scores)\n",
    "\n",
    "    return scores, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model T5\n",
    "\n",
    "* must run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:29.696834Z",
     "iopub.status.busy": "2023-12-13T13:03:29.696807Z",
     "iopub.status.idle": "2023-12-13T13:03:32.201640Z",
     "shell.execute_reply": "2023-12-13T13:03:32.199994Z",
     "shell.execute_reply.started": "2023-12-13T13:03:29.696807Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import easydict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "# from data_utils import ABSADataset, read_line_examples_from_file\n",
    "# from eval_utils import compute_scores\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(tokenizer, type_path, args):\n",
    "    return ABSADataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data_dir=args.dataset,\n",
    "        data_type=type_path,\n",
    "        task = args.task,\n",
    "        max_len=args.max_seq_length,\n",
    "    )\n",
    "\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Fine tune a pre-trained T5 model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams, tfm_model, tokenizer):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = tfm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=batch[\"target_mask\"],\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def on_training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\n",
    "            \"avg_train_loss\": avg_train_loss,\n",
    "            \"log\": tensorboard_logs,\n",
    "            \"progress_bar\": tensorboard_logs,\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.validation_step_outputs.append({\"val_loss\": loss})\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(\n",
    "            [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "        ).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\n",
    "            \"avg_val_loss\": avg_loss,\n",
    "            \"log\": tensorboard_logs,\n",
    "            \"progress_bar\": tensorboard_logs,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "        )\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        outs = self.model.generate(\n",
    "            input_ids=batch[\"source_ids\"].to(device),\n",
    "            attention_mask=batch[\"source_mask\"].to(device),\n",
    "            max_length=128,\n",
    "        )  # num_beams=8, early_stopping=True)\n",
    "\n",
    "        dec = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        # extract_spans_para(task, pred_seqs[i], 'pred')\n",
    "        return dec\n",
    "    \n",
    "    \n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\n",
    "            \"loss\": \"{:.4f}\".format(self.trainer.avg_loss),\n",
    "            \"lr\": self.lr_scheduler.get_last_lr()[-1],\n",
    "        }\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.hparams.train_batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        t_total = (\n",
    "            (\n",
    "                len(dataloader.dataset)\n",
    "                // (\n",
    "                    self.hparams.train_batch_size\n",
    "                    * max(1, len(self.hparams.n_gpu))\n",
    "                )\n",
    "            )\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"dev\", args=self.hparams\n",
    "        )\n",
    "        return DataLoader(\n",
    "            val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4\n",
    "        )\n",
    "\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "        # Log results\n",
    "        for key in sorted(metrics):\n",
    "            if key not in [\"log\", \"progress_bar\"]:\n",
    "                logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "        # Log and save results to file\n",
    "        output_test_results_file = os.path.join(\n",
    "            pl_module.hparams.output_dir, \"test_results.txt\"\n",
    "        )\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, sents, check_inference = False, task = 'asqp'):\n",
    "    \"\"\"\n",
    "    Compute scores given the predictions and gold labels\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{args.n_gpu}\")\n",
    "    model.model.to(device)\n",
    "\n",
    "    model.model.eval()\n",
    "\n",
    "    outputs, targets = [], []\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        # need to push the data to device\n",
    "        outs = model.model.generate(\n",
    "            input_ids=batch[\"source_ids\"].to(device),\n",
    "            attention_mask=batch[\"source_mask\"].to(device),\n",
    "            max_length=128,\n",
    "        )  # num_beams=8, early_stopping=True)\n",
    "\n",
    "        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        target = [\n",
    "            tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            for ids in batch[\"target_ids\"]\n",
    "        ]\n",
    "\n",
    "        outputs.extend(dec)\n",
    "        targets.extend(target)\n",
    "\n",
    "    # Comment\n",
    "\n",
    "    if check_inference:\n",
    "        idx = np.random.randint(0, len(targets), 5)\n",
    "        print(\"\\nPrint some results to check the sanity of generation method:\", '\\n', '-'*30)\n",
    "#         [1, 5, 25, 42, 50]\n",
    "        for i in idx:\n",
    "            try:\n",
    "                print(f'>>Target    : {targets[i]}')\n",
    "                print(f'>>Generation: {outputs[i]}')\n",
    "            except UnicodeEncodeError:\n",
    "                print('Unable to print due to the coding error')\n",
    "        print()\n",
    "\n",
    "\n",
    "    scores, all_labels, all_preds = compute_scores(outputs, targets, sents, task)\n",
    "    results = {\"scores\": scores, \"labels\": all_labels, \"preds\": all_preds}\n",
    "    # pickle.dump(results, open(f\"{args.output_dir}/results-{args.dataset}.pickle\", 'wb'))\n",
    "\n",
    "    return scores, all_labels, all_preds\n",
    "\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        'task': 'tasd',\n",
    "        'dataset': 'mydata',\n",
    "        'model_name_or_path': 't5-base',\n",
    "        'max_seq_length': 128,\n",
    "        'train_batch_size': 16,\n",
    "        'eval_batch_size': 16,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'num_train_epochs': 20,\n",
    "        'seed': 20,\n",
    "        'weight_decay': 0.0,\n",
    "        'adam_epsilon': 1e-8,\n",
    "        'warmup_steps': 0.0,\n",
    "        'n_gpu': 0\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test step\n",
    "* Must run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.264096Z",
     "iopub.status.idle": "2023-12-13T02:48:38.264346Z",
     "shell.execute_reply": "2023-12-13T02:48:38.264233Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.264219Z"
    },
    "id": "EYyqAI2X0POT",
    "outputId": "d1eac65f-ecad-4b45-cb65-d0919da110c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "output_dir = '/content/mydata'\n",
    "\n",
    "print(\"\\n****** Conduct inference on trained checkpoint ******\")\n",
    "\n",
    "    # initialize the T5 model from previous checkpoint\n",
    "print(f\"Load trained model from {output_dir}\")\n",
    "print(\n",
    "        \"Note that a pretrained model is required and `do_true` should be False\"\n",
    "    )\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('/content/mydata')\n",
    "# tfm_model = T5ForConditionalGeneration.from_pretrained('/content/mydata')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('kisejin/T5-ASQP')\n",
    "tfm_model = T5ForConditionalGeneration.from_pretrained('kisejin/T5-ASQP')\n",
    "\n",
    "model = T5FineTuner(args, tfm_model, tokenizer)\n",
    "\n",
    "id_user, sents, _ = read_line_examples_from_file(\n",
    "    f\"/notebooks/My_Project_ABSA-QUAD/data/mydata/test.txt\"\n",
    ")\n",
    "\n",
    "print()\n",
    "test_dataset = ABSADataset(\n",
    "        tokenizer,\n",
    "        data_dir='mydata',\n",
    "        data_type=\"test\",\n",
    "        task = 'tasd',\n",
    "        max_len=128,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)\n",
    "    # print(test_loader.device)\n",
    "\n",
    "    # compute the performance scores\n",
    "scores, all_labels, all_preds = evaluate(test_loader, model, sents, check_inference = True, task = args.task)\n",
    "\n",
    "    # write to file\n",
    "log_file_path = f\"results_log/mydata.txt\"\n",
    "local_time = time.asctime(time.localtime(time.time()))\n",
    "\n",
    "exp_settings = f\"Datset={'mydata'}; Train bs={16}, num_epochs = {20}\"\n",
    "exp_results = f\"F1 = {scores['f1']:.4f}\"\n",
    "\n",
    "log_str = f\"============================================================\\n\"\n",
    "log_str += f\"{local_time}\\n{exp_settings}\\n{exp_results}\\n\\n\"\n",
    "\n",
    "if not os.path.exists(\"./results_log\"):\n",
    "    os.mkdir(\"./results_log\")\n",
    "\n",
    "with open(log_file_path, \"a+\") as f:\n",
    "    f.write(logstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Must run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.266300Z",
     "iopub.status.idle": "2023-12-13T02:48:38.266560Z",
     "shell.execute_reply": "2023-12-13T02:48:38.266462Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.266453Z"
    },
    "id": "tlJi23mKnGE6"
   },
   "outputs": [],
   "source": [
    "# Import sklearn libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def show_result(all_labels, all_preds):\n",
    "    gold_aspect, pred_aspect = [], []\n",
    "    gold_sentiment, pred_sentiment = [], []\n",
    "\n",
    "    for i in range(len(all_labels)):\n",
    "      for label, pred in zip(all_labels[i], all_preds[i]):\n",
    "          if label[0] != '' and pred[0] != '':\n",
    "            gold_aspect.append(label[0])\n",
    "            pred_aspect.append(pred[0])\n",
    "\n",
    "          if label[-1] != '' and pred[-1] != '':\n",
    "            gold_sentiment.append(label[-1])\n",
    "            pred_sentiment.append(pred[-1])\n",
    "\n",
    "    dict_pair = {\n",
    "        'Aspect': [gold_aspect, pred_aspect],\n",
    "        'Sentiment': [gold_sentiment, pred_sentiment]\n",
    "    }\n",
    "\n",
    "    return dict_pair\n",
    "\n",
    "results =show_result(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.267413Z",
     "iopub.status.idle": "2023-12-13T02:48:38.267682Z",
     "shell.execute_reply": "2023-12-13T02:48:38.267571Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.267557Z"
    },
    "id": "mMrBMNsHtGB7",
    "outputId": "ca838d79-052b-4205-c8d0-55d5b8997c0f"
   },
   "outputs": [],
   "source": [
    "# Report Result\n",
    "from IPython.display import display\n",
    "\n",
    "############### Aspect term Report ####################\n",
    "print('ASPECT TERM STATEMENT: \\n')\n",
    "print(f'Classification Report\\n: {classification_report(results[\"Aspect\"][0], results[\"Aspect\"][1])}')\n",
    "print()\n",
    "print(f'Confusion Matrix: \\n')\n",
    "plt.figure(figsize=(10, 10))\n",
    "list_aspect = ['amenity', 'branding', 'experience', 'facility', 'loyalty', 'service']\n",
    "display(sns.heatmap(confusion_matrix(results[\"Aspect\"][0], results[\"Aspect\"][1]), annot=True, fmt='g', yticklabels = list_aspect, xticklabels = list_aspect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.status.busy": "2023-12-13T02:48:38.268639Z",
     "iopub.status.idle": "2023-12-13T02:48:38.268880Z",
     "shell.execute_reply": "2023-12-13T02:48:38.268771Z",
     "shell.execute_reply.started": "2023-12-13T02:48:38.268767Z"
    },
    "id": "8EW5eQZpuYN3",
    "outputId": "af9c7cae-511f-4444-b649-9cb526cd5f8b"
   },
   "outputs": [],
   "source": [
    "############### Sentiment Report ####################\n",
    "print('SENTIMENT STATEMENT: \\n')\n",
    "print(f'Classification Report\\n: {classification_report(results[\"Sentiment\"][0], results[\"Sentiment\"][1])}')\n",
    "print()\n",
    "print(f'Confusion Matrix: \\n')\n",
    "plt.figure(figsize=(10, 10))\n",
    "list_sentiment = ['negative', 'neutral', 'positive']\n",
    "display(sns.heatmap(confusion_matrix(results[\"Sentiment\"][0], results[\"Sentiment\"][1]), annot=True, fmt='g', yticklabels = list_sentiment, xticklabels = list_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T08:29:11.994320Z",
     "iopub.status.busy": "2023-12-13T08:29:11.993218Z",
     "iopub.status.idle": "2023-12-13T09:11:39.767185Z",
     "shell.execute_reply": "2023-12-13T09:11:39.764339Z",
     "shell.execute_reply.started": "2023-12-13T08:29:11.994258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Predict on trained checkpoint ******\n",
      "Load trained model\n",
      "Note that a pretrained model is required and `do_true` should be False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd959a81fc422f8d1252049b955b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to predict: Wed Dec 13 09:11:39 2023\n",
      "**************************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logstr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results_log\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 86\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mlogstr\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logstr' is not defined"
     ]
    }
   ],
   "source": [
    "args = easydict.EasyDict({\n",
    "        'task': 'tasd',\n",
    "        'dataset': 'mydata',\n",
    "        'model_name_or_path': 't5-base',\n",
    "        'max_seq_length': 128,\n",
    "        'train_batch_size': 16,\n",
    "        'eval_batch_size': 16,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'num_train_epochs': 20,\n",
    "        'seed': 20,\n",
    "        'weight_decay': 0.0,\n",
    "        'adam_epsilon': 1e-8,\n",
    "        'warmup_steps': 0.0,\n",
    "        'n_gpu': 0\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "output_dir = '/content/mydata'\n",
    "\n",
    "print(\"\\n****** Predict on trained checkpoint ******\")\n",
    "\n",
    "    # initialize the T5 model from previous checkpoint\n",
    "print(f\"Load trained model\")\n",
    "print(\n",
    "        \"Note that a pretrained model is required and `do_true` should be False\"\n",
    ")\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('/content/mydata')\n",
    "# tfm_model = T5ForConditionalGeneration.from_pretrained('/content/mydata')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('kisejin/T5-ASQP')\n",
    "tfm_model = T5ForConditionalGeneration.from_pretrained('kisejin/T5-ASQP')\n",
    "\n",
    "model = T5FineTuner(args, tfm_model, tokenizer)\n",
    "\n",
    "id_user, sents, _ = read_line_examples_from_file(\n",
    "    f\"/notebooks/predict_file.txt\"\n",
    ")\n",
    "\n",
    "print()\n",
    "predict_dataset = ABSADataset_nolabel(\n",
    "        tokenizer,\n",
    "        data_dir='/notebooks/predict_file.txt',\n",
    "        data_type=\"test\",\n",
    "        task = 'tasd',\n",
    "        max_len=128,\n",
    ")\n",
    "predict_loader = DataLoader(predict_dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "\n",
    "# compute the performance scores\n",
    "# scores, all_labels, all_preds = evaluate(test_loader, model, sents, check_inference = True, task = args.task)\n",
    "\n",
    "# predict time\n",
    "\n",
    "trainer_params = dict(\n",
    "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "        # gpus=args.n_gpu,\n",
    "        devices=\"auto\",\n",
    "        gradient_clip_val=1.0,\n",
    "        # max_epochs=args.num_train_epochs,\n",
    "        callbacks=[LoggingCallback()],\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(**trainer_params)\n",
    "\n",
    "pred = trainer.predict(model, predict_loader)\n",
    "# pred = model.predict_step(predict_loader)\n",
    "\n",
    "\n",
    "# write to file\n",
    "log_file_path = f\"results_log/predictdata.txt\"\n",
    "local_time = time.asctime(time.localtime(time.time()))\n",
    "\n",
    "print('Time to predict:', local_time)\n",
    "print('*'*50)\n",
    "\n",
    "if not os.path.exists(\"./results_log\"):\n",
    "    os.mkdir(\"./results_log\")\n",
    "\n",
    "with open(log_file_path, \"a+\") as f:\n",
    "    f.write(logstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:48.588278Z",
     "iopub.status.busy": "2023-12-13T13:03:48.587348Z",
     "iopub.status.idle": "2023-12-13T13:03:48.611301Z",
     "shell.execute_reply": "2023-12-13T13:03:48.607655Z",
     "shell.execute_reply.started": "2023-12-13T13:03:48.588224Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_time(data_loader, model, task = 'asqp'):\n",
    "    \"\"\"\n",
    "    Predict time for new dataset\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{args.n_gpu}\")\n",
    "    model.model.to(device)\n",
    "\n",
    "    model.model.eval()\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        # need to push the data to device\n",
    "        outs = model.model.generate(\n",
    "            input_ids=batch[\"source_ids\"].to(device),\n",
    "            attention_mask=batch[\"source_mask\"].to(device),\n",
    "            max_length=128\n",
    "        )  # num_beams=8, early_stopping=True)\n",
    "\n",
    "        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        outputs.extend(dec)\n",
    "\n",
    "\n",
    "    # pickle.dump(results, open(f\"{args.output_dir}/results-{args.dataset}.pickle\", 'wb'))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T09:13:58.334903Z",
     "iopub.status.busy": "2023-12-13T09:13:58.333244Z",
     "iopub.status.idle": "2023-12-13T09:14:03.745186Z",
     "shell.execute_reply": "2023-12-13T09:14:03.744502Z",
     "shell.execute_reply.started": "2023-12-13T09:13:58.334815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amenity is great because amazing location is great [SSEP] amenity is great because everything is near by is great [SSEP] service is great because there are free bikes available even with a child seat is great', 'service is great because friendly staff is great [SSEP] experience is great because nice hostel to get to know other travelers is great [SSEP] amenity is great because perfect location is great', 'experience is great because our stay at boutique lodge was probably the best during our month long stay in vietnam is great [SSEP] facility is great because our room was spacious and comfortable with outside seating facing the garden area and pool is great [SSEP] facility is great because the saltwater pool is luxurious is great [SSEP] experience is great because especially since this was our first warm weather place we visited in vietnam as we were traveling north to south is great [SSEP] experience is great because the early morning water market trip on taun s boat was followed up with a tour of the incredible', 'facility is great because i could sleep on the bed i could have a shower and use the toilet is great [SSEP] service is bad because when i arrived they had double booked my room and after taking my money walked me to a love hotel 20 minutes away is bad [SSEP] service is bad because the next day i was moved back to the original hostel and told my room would be ready at 2pm the receptionist went upstairs to begin cleaning it and at 2 30pm it was unsurprisingly not ready either is bad', 'service is great because very good breakfast is great [SSEP] service is great because staff was very friendly and helpful at all times is great [SSEP] facility is great because pool and courtyard were a delight is great [SSEP] amenity is great because good location close to the center but not too close is great [SSEP] experience is great because hoi an is a magical place at night is great [SSEP] facility is bad because my suite with river view didn  t have a balcony as stated something i was looking forward to is bad [SSEP] experience is ok', 'service is great because both staff the property are amazing is great [SSEP] experience is great because breathtaking view if you are on a high floor with the water view is great', 'facility is bad because the location it wasn t clean there were stains on the sheets the shower was superficially clean is bad [SSEP] service is bad because on booking they offer late check out but tell you later that you have to pay the whole price for a later check out is bad', 'experience is great because we are so glad to have stayed at this property in a village that s a short taxi ride away from sapa town center is great [SSEP] amenity is great because while the sapa town center is overbuilt and over commercialized is great [SSEP] amenity is great because the hotel is in a laid back village that s right in and around rice fields is great [SSEP] experience is great because we loved looking out of room and staring right at the most gorgeous view of the rice fields is great [SSEP] service is great because', 'experience is great because charming homestay tucked away on the outskirts of mai chau is great [SSEP] experience is great because quiet clean is great [SSEP] service is great because the staff were very helpful is great [SSEP] service is great because delicious meal is great [SSEP] experience is great because great value for price is great', 'facility is bad because room small is bad', 'service is great because the staff arranged an early check in for me since the room is already available is great [SSEP] experience is great because the view from the room is totally amazing is great [SSEP] experience is great because i regret having a short stay because the view itself is worth the value i paid for is great [SSEP] facility is bad because the room smelled like someone just smoked a cigarette inside is bad [SSEP] facility is bad because not sure if it was because of poor vents is bad [SSEP] facility is ok because would have', 'amenity is great because close to beach and restaurants is great [SSEP] facility is great because nice size room is great [SSEP] amenity is great because markets and street food within 3 minutes walk is great', 'amenity is great because beautiful hotel in a great location is great [SSEP] amenity is great because easy walking distance to the old town is great [SSEP] facility is great because rooms were very comfortable and had everything we needed is great [SSEP] facility is great because beds and pillows were also very comfortable is great [SSEP] experience is great because stunning view of the river and rooms had balconies is great [SSEP] service is great because the hotel were very gracious and did not charge us for the remaining two nights is great [SSEP] service is great because receptionists were', 'facility is great because clean spacious room with super comfy big bed and hot powerful shower is great [SSEP] service is great because staff are lovely and helpful is great [SSEP] experience is great because great value is great [SSEP] loyalty is great because i highly recommend it is great [SSEP] service is great because they also organised a taxi for us from the airport which was nice to not have to navigate in the middle of the night is great [SSEP] experience is bad because bring earplugs because it can get noisy is bad [SSEP] amenity is bad because the', 'facility is great because the hotel itself is very clean and unexpectedly huge is great [SSEP] amenity is great because strategic location to walk around while exploring the city is great [SSEP] facility is bad because the lobby was really small and only one lift available is bad [SSEP] amenity is great because located very near to the party nightclub streets hence can be noisy is great [SSEP] facility is ok because bring your earplugs if you want to sleep early is ok', 'facility is great because the rooftop pool is great [SSEP] amenity is great because the location is great [SSEP] service is great because the staff were excellent is great [SSEP] facility is ok because the hotel is not brand new but that was expected for the price is ok [SSEP] experience is great because to be fair it beat our expectations and we will happily stay at alani next time we come to da nang is great', 'experience is great because everything was better than i could have hoped for is great [SSEP] facility is great because the room was spacious the bed was super comfortable the view was nice is great [SSEP] service is great because breakfast was varied and quite tasty is great [SSEP] facility is great because pool and exercise room were both enjoyed is great [SSEP] service is great because the staff were all pleasant and helpful is great [SSEP] loyalty is great because i look forward to coming back is great', 'service is great because staff was very friendly is great', 'service is great because particularly management tony his family is great [SSEP] facility is bad because wi fi internet#### is bad', 'experience is great because such a great find is great [SSEP] amenity is great because perfectly located near the centre is great [SSEP] facility is great because i loved having the pool next to my room is great [SSEP] service is great because staff was very friendly and helpful with anything i needed is great', 'facility is great because the rooftop bar was really fantastic and the cocktails were cheap is great [SSEP] amenity is great because the location is great is great [SSEP] experience is great because the view was the best thing about staying at ellios is great [SSEP] service is great because the people serving were nice and the buffet was good is great [SSEP] facility is great because aircon was great is great [SSEP] facility is bad because there isn t a jacuzzi massage area or gym like they said there is is bad [SSEP] facility is bad because', 'service is great because the owners were very friendly and kind they gave us a lot of helpful information and helped organize pick up and drop offs for us to and from the train station at a very reasonable price is great [SSEP] facility is great because the rooms were exceptionally clean and the facilities were perfect is great [SSEP] loyalty is great because would definitely recommend and would absolutely stay again if i ever return to hoi an is great [SSEP] experience is great because there was nothing i didn t like i had an amazing time staying here is great', 'service is great because the front desk personal were always helpful professional and polite is great [SSEP] service is great because the gentleman who park the bikes are very polite and professional is great', 'amenity is great because excellent location is great [SSEP] service is great because very cooperative and helpful staff is great [SSEP] service is great because we were upgraded to a better room as there was initially a problem with ac in room and even got a free drop off to airport is great [SSEP] facility is great because the paintings and artifacts in lobby and elsewhere in hotel are lovely is great', 'service is great because host and views were great is great [SSEP] service is great because loved that you could pick up a scoot from the home stay and can easily access all that ninh binh has to offer is great', '', 'facility is great because room was clean is great [SSEP] facility is great because smelly bedsheets i think they don t really change them after a checkout is great [SSEP] facility is bad because noise from the outside and the elevator is bad [SSEP] facility is bad because bathroom old and not very appealing furthermore getting unavoidably wet when showering as the shower head is in between the sink and the toilet is bad [SSEP] amenity is bad because not really centrally located is bad [SSEP] branding is bad because looks better on the pictures', 'service is great because staff and room was nice bed was comfortable is great [SSEP] amenity is great because the location is great [SSEP] experience is great because i ve never heard anything like it do not stay here if youre looking to sleep is great [SSEP] experience is ok because only if you re there to party is ok', 'service is great because courteous staffs is great [SSEP] facility is great because quiet and clean room is great [SSEP] service is great because excellent breakfast is great [SSEP] experience is great because calm peaceful relaxing is great [SSEP] experience is great because we thoroughly enjoyed the experience is great', 'experience is great because i loveeeee this hotel is great [SSEP] experience is great because i think this is the best shared bed hostel experience i ever had is great', 'service is great because staff are friendly and helpful is great [SSEP] facility is ok because room little small than expected is ok', 'experience is great because it was peaceful is great [SSEP] experience is great because much needed calmness is great [SSEP] service is great because free bicycle band good climate is great [SSEP] facility is ok because the bathroom could be better is ok [SSEP] facility is bad because the ac was old and was difficult to operate properly is bad']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained('kisejin/T5-ASQP')\n",
    "# tfm_model = T5ForConditionalGeneration.from_pretrained('kisejin/T5-ASQP')\n",
    "\n",
    "# model = T5FineTuner(args, tfm_model, tokenizer)\n",
    "\n",
    "\n",
    "# device = torch.device(f\"cuda:{args.n_gpu}\")\n",
    "# model.model.to(device)\n",
    "\n",
    "# model.model.eval()\n",
    "\n",
    "# outputs = []\n",
    "\n",
    "# for batch in tqdm(predict_loader):\n",
    "#         # need to push the data to device\n",
    "#     outs = model.model.generate(\n",
    "#                 input_ids=batch[\"source_ids\"].to(device),\n",
    "#                 attention_mask=batch[\"source_mask\"].to(device),\n",
    "#                 max_length=128,\n",
    "#     )  # num_beams=8, early_stopping=True)\n",
    "\n",
    "#     dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "#     outputs.extend(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T13:03:55.604102Z",
     "iopub.status.busy": "2023-12-13T13:03:55.603226Z",
     "iopub.status.idle": "2023-12-13T17:46:46.522559Z",
     "shell.execute_reply": "2023-12-13T17:46:46.520766Z",
     "shell.execute_reply.started": "2023-12-13T13:03:55.604072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce6c93d19394eb7bef9e3eea0b87b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326926b605d142d8b934c9870d95f132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676bc486ea10474e843ac1117a44277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc540e8572e4bb193a1fd2335065b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7c48ae815346538a91de2f4fae2a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b84deaa3ab410dad7ed6a42e83d267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b826fcd7f2664ee3b94fffaf4f29b818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1998/1998 [4:40:00<00:00,  8.41s/it]  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('kisejin/T5-ASQP')\n",
    "tfm_model = T5ForConditionalGeneration.from_pretrained('kisejin/T5-ASQP')\n",
    "\n",
    "model = T5FineTuner(args, tfm_model, tokenizer)\n",
    "\n",
    "predict_dataset = ABSADataset_nolabel(\n",
    "        tokenizer,\n",
    "        data_dir='/notebooks/predict_file.txt',\n",
    "        data_type=\"test\",\n",
    "        task = 'tasd',\n",
    "        max_len=128,\n",
    ")\n",
    "\n",
    "predict_loader = DataLoader(predict_dataset, batch_size=128, num_workers=8)\n",
    "\n",
    "\n",
    "pred = predict_time(predict_loader, model, task = 'tasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-13T17:56:07.945891Z",
     "iopub.status.busy": "2023-12-13T17:56:07.945241Z",
     "iopub.status.idle": "2023-12-13T17:56:08.020040Z",
     "shell.execute_reply": "2023-12-13T17:56:08.018888Z",
     "shell.execute_reply.started": "2023-12-13T17:56:07.945859Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'pred': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-13T17:58:33.546816Z",
     "iopub.status.busy": "2023-12-13T17:58:33.545409Z",
     "iopub.status.idle": "2023-12-13T17:58:33.658349Z",
     "shell.execute_reply": "2023-12-13T17:58:33.656445Z",
     "shell.execute_reply.started": "2023-12-13T17:58:33.546777Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnotebook/predict_output.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:3721\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3710\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3712\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3713\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3714\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3718\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3719\u001b[0m )\n\u001b[0;32m-> 3721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py:735\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 735\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py:598\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    596\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'notebook'"
     ]
    }
   ],
   "source": [
    "df.to_csv('notebook/predict_output.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
